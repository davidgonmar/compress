import torch
from torch import nn
from enum import Enum
from abc import ABC, abstractmethod


class AbstractGrouper(ABC):

    @abstractmethod
    def group(self, x: torch.Tensor) -> torch.Tensor:
        pass

    @abstractmethod
    def ungroup(self, x: torch.Tensor, orig: torch.Tensor) -> torch.Tensor:
        pass

    def __repr__(self):
        return self.__class__.__name__


# One tensor -> one param
class PerTensor(AbstractGrouper):

    def group(self, x: torch.Tensor) -> torch.Tensor:
        return x.reshape(-1, 1)

    def ungroup(self, x: torch.Tensor, orig: torch.Tensor) -> torch.Tensor:
        return x.reshape(orig.shape)


# One row -> one param
class LinearPerRow(AbstractGrouper):

    def group(self, x: torch.Tensor) -> torch.Tensor:
        return x.reshape(x.shape[0], -1).T

    def ungroup(self, x: torch.Tensor, orig: torch.Tensor) -> torch.Tensor:
        return x.T.reshape(orig.shape)


# One column -> one param
class LinearPerColumn(AbstractGrouper):

    def group(self, x: torch.Tensor) -> torch.Tensor:
        return x.reshape(-1, x.shape[1])

    def ungroup(self, x: torch.Tensor, orig: torch.Tensor) -> torch.Tensor:
        return x.reshape(orig.shape)


# One channel -> one param
class ConvWeightsPerOutChannel(AbstractGrouper):

    def group(self, x: torch.Tensor) -> torch.Tensor:
        return x.reshape(x.shape[0], -1).T

    def ungroup(self, x: torch.Tensor, orig: torch.Tensor) -> torch.Tensor:
        return x.T.reshape(orig.shape)


# a linear has shape [out_features, in_features]
# and computation is X @ W^T
# so each out channel of x is generated by a column of W^t -> row of W
class LinearWeightsPerOutChannel(AbstractGrouper):

    def group(self, x: torch.Tensor) -> torch.Tensor:
        assert x.ndim == 2, "Linear weights should be 2D"
        return x.T

    def ungroup(self, x: torch.Tensor, orig: torch.Tensor) -> torch.Tensor:
        assert orig.ndim == 2, "Linear weights should be 2D"
        return x.T


class ConvActsPerOutChannel(AbstractGrouper):

    def group(self, x: torch.Tensor) -> torch.Tensor:
        assert x.ndim == 4, "Conv acts should be 4D"
        y = x.reshape(
            x.shape[0], x.shape[1], -1
        )  # shape [batch_size, out_channels, H*W]
        y = y.permute(0, 2, 1)  # shape [batch_size, H*W, out_channels]
        y = y.reshape(-1, x.shape[1])  # shape [batch_size*H*W, out_channels]
        return y

    def ungroup(self, x: torch.Tensor, orig: torch.Tensor) -> torch.Tensor:
        assert orig.ndim == 4, "Conv acts should be 4D"
        bs, c, h, w = orig.shape
        y = x.reshape(bs, h, w, c)
        y = y.permute(0, 3, 1, 2)  # shape [batch_size, out_channels, H, W]
        assert y.shape == orig.shape, f"Expected {orig.shape}, got {y.shape}"
        return y


class IntAffineQuantizationMode(Enum):
    SYMMETRIC = "SYMMETRIC"
    ASYMMETRIC = "ASYMMETRIC"
    STATISTICS_AWARE_BINNING_SYMMETRIC = "STATISTICS_AWARE_BINNING_SYMMETRIC"  # from https://arxiv.org/abs/1807.06964, it's symmetric
    STATISTICS_AWARE_BINNING_ASYMMETRIC = (
        "STATISTICS_AWARE_BINNING_ASYMMETRIC"  # extension from the above
    )
    ENTROPY_SYMMETRIC = "ENTROPY_SYMMETRIC"
    LSQ_INITIALIZATION = "LSQ_INITIALIZATION"


class IntAffineQuantizationSpec:
    nbits: int
    signed: bool
    quant_mode: IntAffineQuantizationMode
    mode_args: dict = {}

    grouper: AbstractGrouper
    # grouper takes a tensor of any shape and returns a tensor of shape [group_d, n_groups]
    # each group has its own parameters

    def __post_init__(self):
        if self.quant_mode in [
            IntAffineQuantizationMode.STATISTICS_AWARE_BINNING_SYMMETRIC,
            IntAffineQuantizationMode.STATISTICS_AWARE_BINNING_ASYMMETRIC,
        ]:
            assert (
                self.signed
            ), "Statistics-aware binning only supports signed quantization"

        if self.quant_mode is IntAffineQuantizationMode.SYMMETRIC:
            if self.mode_args.get("abs_percentile", None) is None:
                self.mode_args["abs_percentile"] = 100.0  # absmax by default
        elif self.quant_mode is IntAffineQuantizationMode.ASYMMETRIC:
            if self.mode_args.get("lower_percentile", None) is None:
                self.mode_args["lower_percentile"] = 0.0
            if self.mode_args.get("upper_percentile", None) is None:
                self.mode_args["upper_percentile"] = 100.0

    @property
    def qmin(self):
        return -(1 << (self.nbits - 1)) if self.signed else 0

    @property
    def qmax(self):
        return (1 << (self.nbits - 1)) - 1 if self.signed else (1 << self.nbits) - 1

    def get_dtype(self):
        return {
            (8, False): torch.uint8,
            (8, True): torch.int8,
            (16, False): torch.uint16,
            (16, True): torch.int16,
        }.get((self.nbits, self.signed), torch.float32)

    def from_dtype(dtype: torch.dtype | str):
        if dtype in ["int2, int4"]:
            return IntAffineQuantizationSpec(2 if dtype == "int2" else 4, True)
        if isinstance(dtype, str):
            dtype = torch.dtype(dtype)
        return {
            torch.uint8: IntAffineQuantizationSpec(8, False),
            torch.int8: IntAffineQuantizationSpec(8, True),
            torch.uint16: IntAffineQuantizationSpec(16, False),
            torch.int16: IntAffineQuantizationSpec(16, True),
        }[dtype]

    def __init__(
        self,
        nbits: int,
        signed: bool,
        quant_mode: IntAffineQuantizationMode,
        grouper: AbstractGrouper = PerTensor,
        **kwargs,
    ):
        self.nbits = nbits
        self.signed = signed
        self.quant_mode = quant_mode
        self.mode_args = kwargs
        self.grouper = grouper

    def __repr__(self):
        return f"IntAffineQuantizationSpec(nbits={self.nbits}, signed={self.signed}, quant_mode={self.quant_mode}, mode_args={self.mode_args}, grouper={self.grouper})"


class IntAffineQuantizationInfo(nn.Module):
    spec: IntAffineQuantizationSpec
    scale: torch.Tensor
    zero_point: torch.Tensor | None

    @property
    def nbits(self):
        return self.spec.nbits

    @property
    def signed(self):
        return self.spec.signed

    @property
    def qmin(self):
        return self.spec.qmin

    @property
    def qmax(self):
        return self.spec.qmax

    @property
    def group_dims(self):
        return self.spec.group_dims

    def get_dtype(self):
        return self.spec.get_dtype()

    def __init__(
        self,
        spec: IntAffineQuantizationSpec,
        scale: torch.Tensor,
        zero_point: torch.Tensor | None = None,
    ):
        super().__init__()
        self.spec = spec
        self.scale = nn.Parameter(scale, requires_grad=False)
        # where scale is 0, make it one
        self.scale.data[self.scale == 0] = 1.0
        # clamp zero_point to be in the range of qmin and qmax
        zero_point = (
            torch.clamp(zero_point, spec.qmin, spec.qmax)
            if zero_point is not None
            else None
        )
        self.zero_point = (
            nn.Parameter(zero_point, requires_grad=False)
            if zero_point is not None
            else None
        )

    def __repr__(self):
        return f"IntAffineQuantizationInfo(spec={self.spec}, scale={self.scale}, zero_point={self.zero_point})"


class STERound(torch.autograd.Function):
    @staticmethod
    def forward(ctx, x):
        return torch.round(x)

    @staticmethod
    def backward(ctx, grad_output):
        return grad_output


def ste_round(x):
    return STERound.apply(x)


class STEFloor(torch.autograd.Function):
    @staticmethod
    def forward(ctx, x):
        return torch.floor(x)

    @staticmethod
    def backward(ctx, grad_output):
        return grad_output


def ste_floor(x):
    return STEFloor.apply(x)


def _or0(x):
    if x is None:
        return 0
    return x


def quantize(x: torch.Tensor, info: IntAffineQuantizationInfo):
    return info.spec.grouper.ungroup(
        torch.clamp(
            ste_round(info.spec.grouper.group(x) / info.scale + _or0(info.zero_point)),
            info.qmin,
            info.qmax,
        ).to(info.get_dtype()),
        x,
    )


def dequantize(x: torch.Tensor, info: IntAffineQuantizationInfo):
    return info.spec.grouper.ungroup(
        info.scale
        * (info.spec.grouper.group(x.to(torch.float32)) - _or0(info.zero_point)),
        x,
    )


def fake_quantize(x: torch.Tensor, info: IntAffineQuantizationInfo):
    return dequantize(quantize(x, info), info)
